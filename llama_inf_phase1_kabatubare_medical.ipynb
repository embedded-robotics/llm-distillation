{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will deal with the batch inference of medical healthcare question/answer dataset using LLaMA model.\n",
    "\n",
    "https://huggingface.co/datasets/Kabatubare/medical\n",
    "\n",
    "After the inference, the LLaMA answers will be evaluated using BLEU/ROUGE against the groundtruth answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Medical Healthcare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Context', 'Question', 'Answer'],\n",
       "    num_rows: 23437\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"Kabatubare/medical\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading one instance of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Context': 'You are a medical knowledge assistant trained to provide information and guidance on various health-related topics.',\n",
       " 'Question': 'my 5 1/2-year-old son displays adhd symptoms for 20 days then for 10 days he has none. is it adhd or another condition?',\n",
       " 'Answer': \"adhd and bipolar mood disorder (bmd) can coexist or be mistaken for one another. bmd usually is not diagnosed until young adulthood. however studies have shown that the earlier a person is diagnosed with bmd the more likely he is to have been diagnosed with adhd previously. in this case i would just like to reiterate that there is not enough information to discuss either possibility for your son. you mentioned that he becomes hyperactive for 3 weeks but not what his behaviors are like during those 10 days. you also do not mention irritability or mood swings just adhd symptoms. keep documenting the symptoms you are concerned about including what goes on in the home and at school when you see changes in behavior (do you work those weeks does he visit a relative or have a different aide in the classroom). you also mentioned that this began 7 months ago. i would also urge you to think about what also might have changed in your son's life about that time. consulting your pediatrician or a child psychiatrist should help you unravel the mystery of your son's behavior.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the LLaMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hf_token.key', 'r') as f:\n",
    "    hf_token = f.read()\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.model_max_length = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\") # Must be float32 for MacBooks!\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # Updating the model config to use the special pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the `eos` token as the terminator to finish the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the function to get the LLaMA Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_response(question_inputs: str):\n",
    "    \n",
    "    llama_inputs = [[{\"role\": \"system\", \"content\": \"You are a medical knowledge assistant trained to provide information and guidance on various health-related topics.\"},\n",
    "                     {\"role\": \"user\", \"content\": question}] for question in question_inputs]\n",
    "\n",
    "    texts = tokenizer.apply_chat_template(llama_inputs, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(texts, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "    temp_texts = tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)\n",
    "    \n",
    "    gen_tokens = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=4096, \n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        # top_p=0.9\n",
    "    )\n",
    "\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    gen_text = [i[len(temp_texts[idx]):] for idx, i in enumerate(gen_text)]\n",
    "    \n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "dataset_questions = dataset['Question']\n",
    "dataset_answers = dataset['Answer']\n",
    "batch_indices = np.arange(0, len(dataset_questions), batch_size)\n",
    "if batch_indices[-1] != len(dataset_questions):\n",
    "    batch_indices = np.append(batch_indices, len(dataset_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the batch inference to get responses from LLaMA\n",
    "\n",
    "Only run the following cell when new inferences are needed. Otherwise, keep it commented and move onto next section for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:56<00:00, 116.94s/it]\n"
     ]
    }
   ],
   "source": [
    "question_list = []\n",
    "answer_list = []\n",
    "llama_responses = []\n",
    "for i in tqdm(range(0, len(batch_indices) - 1)):\n",
    "    questions_input = dataset_questions[batch_indices[i]:batch_indices[i+1]]\n",
    "    orig_answers = dataset_questions[batch_indices[i]:batch_indices[i+1]]\n",
    "    llama_resp = get_llama_response(questions_input)\n",
    "    \n",
    "    question_list = question_list + questions_input\n",
    "    answer_list = answer_list + orig_answers\n",
    "    llama_responses = llama_responses + llama_resp\n",
    "\n",
    "with open('kabatubare_medical/questions.pkl', 'wb') as file:\n",
    "    pickle.dump(question_list, file)\n",
    "    \n",
    "with open('kabatubare_medical/answers.pkl', 'wb') as file:\n",
    "    pickle.dump(answer_list, file)\n",
    "\n",
    "with open('kabatubare_medical/llama_resp.pkl', 'wb') as file:\n",
    "    pickle.dump(llama_responses, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluaing the LLaMA Responses\n",
    "\n",
    "Reading the questions, answers and LLaMA Responses for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kabatubare_medical/questions.pkl', 'rb') as file:\n",
    "    question_list = pickle.load(file)\n",
    "    \n",
    "with open('kabatubare_medical/answers.pkl', 'rb') as file:\n",
    "    answer_list = pickle.load(file)\n",
    "\n",
    "with open('kabatubare_medical/llama_resp.pkl', 'rb') as file:\n",
    "    llama_responses = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
