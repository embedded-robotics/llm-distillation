{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA Supervised Fine-Tuning\n",
    "\n",
    "This document will take the answers of GPT-4o on the Kababutare Medical Dataset and then fine-tune the LLaMA Model on those answers.\n",
    "\n",
    "The purpose of this exercise is to test whether the LLaMA fine-tuning is able to distill the knowledge of GPT-4o and improve the performance on the open-ended question/answering related to healthcare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported, train_on_responses_only\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Question and Answer Pairs from Phase 1 of GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gpt_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my 5 1/2-year-old son displays adhd symptoms f...</td>\n",
       "      <td>Itâ€™s important to remember that only a qualifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my son has add and mild autism. he has been su...</td>\n",
       "      <td>Weight management can be a concern for childre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my son is 13 and is depressed. he has been tak...</td>\n",
       "      <td>I'm really sorry to hear that your son is feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my 17-year-old has stopped taking concerta aft...</td>\n",
       "      <td>When a person, especially a teenager, stops ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've been taking respa-ar for allergies. i can...</td>\n",
       "      <td>Resp-A-R is a combination medication commonly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23432</th>\n",
       "      <td>how can accidental of acetaminophen overdose b...</td>\n",
       "      <td>Accidental acetaminophen overdose is a signifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23433</th>\n",
       "      <td>what should i do if i take an overdose of maxalt?</td>\n",
       "      <td>If you suspect that you have taken an overdose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23434</th>\n",
       "      <td>what do i do in case of an overdose of relpax?</td>\n",
       "      <td>If you suspect an overdose of Relpax (eletript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23435</th>\n",
       "      <td>is overdose with acetaminophen usually acciden...</td>\n",
       "      <td>Overdoses of acetaminophen (also known as para...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23436</th>\n",
       "      <td>how does an overdose of acetaminophen cause li...</td>\n",
       "      <td>Acetaminophen (also known as paracetamol) is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23437 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "0      my 5 1/2-year-old son displays adhd symptoms f...   \n",
       "1      my son has add and mild autism. he has been su...   \n",
       "2      my son is 13 and is depressed. he has been tak...   \n",
       "3      my 17-year-old has stopped taking concerta aft...   \n",
       "4      i've been taking respa-ar for allergies. i can...   \n",
       "...                                                  ...   \n",
       "23432  how can accidental of acetaminophen overdose b...   \n",
       "23433  what should i do if i take an overdose of maxalt?   \n",
       "23434     what do i do in case of an overdose of relpax?   \n",
       "23435  is overdose with acetaminophen usually acciden...   \n",
       "23436  how does an overdose of acetaminophen cause li...   \n",
       "\n",
       "                                            gpt_response  \n",
       "0      Itâ€™s important to remember that only a qualifi...  \n",
       "1      Weight management can be a concern for childre...  \n",
       "2      I'm really sorry to hear that your son is feel...  \n",
       "3      When a person, especially a teenager, stops ta...  \n",
       "4      Resp-A-R is a combination medication commonly ...  \n",
       "...                                                  ...  \n",
       "23432  Accidental acetaminophen overdose is a signifi...  \n",
       "23433  If you suspect that you have taken an overdose...  \n",
       "23434  If you suspect an overdose of Relpax (eletript...  \n",
       "23435  Overdoses of acetaminophen (also known as para...  \n",
       "23436  Acetaminophen (also known as paracetamol) is a...  \n",
       "\n",
       "[23437 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_inf_data_phase1 = pd.DataFrame()\n",
    "ques_list = []\n",
    "gpt_resp_list = []\n",
    "\n",
    "with open('kabatubare_medical_gpt4omini_qa_pairs.jsonl', 'rb') as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line)\n",
    "        ques_list.append(json_object['Question'])\n",
    "        gpt_resp_list.append(json_object['Answer'])\n",
    "\n",
    "gpt_inf_data_phase1['question'] = ques_list\n",
    "gpt_inf_data_phase1['gpt_response'] = gpt_resp_list\n",
    "gpt_inf_data_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the HuggingFace Dataset from Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'gpt_response'],\n",
       "        num_rows: 21093\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'gpt_response'],\n",
       "        num_rows: 2344\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(gpt_inf_data_phase1)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.2.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 1. Max memory: 47.413 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 4096,\n",
    "    load_in_4bit = False, # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = True, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    dtype=None, #None for auto-detection. Can be torch.bfloat16 or torch.float16 (will be automatically detected)\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the PEFT settings for the model\n",
    "\n",
    "https://huggingface.co/blog/damjan-k/rslora\\\n",
    "https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, #max_full_rank=64 by default in FastLanguageModel\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64, #scaling_factor = lora_alpha/r. If we select lora_alpha = 2 * r then it will multiply the adapter weights by 2 which can be un-ncessary\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    use_rslora = True,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forming the chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the chat template\n",
    "def format_chat_template(example):\n",
    "        \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health\"},\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": example['gpt_response']}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21093/21093 [00:03<00:00, 5412.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2344/2344 [00:00<00:00, 6010.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_formatted = dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 31 Mar 2025\n",
      "\n",
      "You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "because i am on ssdi i receive medicare. do i have to get separate health care coverage under obamacare?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "If you are receiving Social Security Disability Insurance (SSDI) and are eligible for Medicare, you do not need to obtain separate health care coverage under the Affordable Care Act (commonly referred to as \"Obamacare\"). Medicare is considered minimum essential coverage, which means that having Medicare satisfies the health insurance requirement under the Affordable Care Act.\n",
      "\n",
      "However, you may choose to enroll in a Marketplace plan if you want additional coverage or if there are specific services that Medicare does not cover. Before making any changes, it is recommended to carefully evaluate your options, as you don't want to risk losing your existing coverage.\n",
      "\n",
      "Always consider consulting with a healthcare navigator or a benefits counselor who can help you understand the implications and provide tailored advice based on your specific situation.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_formatted['train']['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the TRL SFTTrainer and related Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21093/21093 [00:31<00:00, 671.28 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2344/2344 [00:05<00:00, 407.81 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# full_model_path = \"./llama32-sft-full-kabatubare\" #use for full finetuning\n",
    "peft_model_path = \"./llama32-sft-peft-kabatubare\" #use for LoRA based fine-tuning\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=peft_model_path,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        # gradient_accumulation_steps=4,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        seed = 42,\n",
    "        report_to = \"none\",\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset=dataset_formatted[\"train\"],\n",
    "    eval_dataset=dataset_formatted[\"test\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 4096,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer), #only use when using train_on_responses_only()\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'gpt_response', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 21093\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 31 Mar 2025\n",
      "\n",
      "You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "because i am on ssdi i receive medicare. do i have to get separate health care coverage under obamacare?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "If you are receiving Social Security Disability Insurance (SSDI) and are eligible for Medicare, you do not need to obtain separate health care coverage under the Affordable Care Act (commonly referred to as \"Obamacare\"). Medicare is considered minimum essential coverage, which means that having Medicare satisfies the health insurance requirement under the Affordable Care Act.\n",
      "\n",
      "However, you may choose to enroll in a Marketplace plan if you want additional coverage or if there are specific services that Medicare does not cover. Before making any changes, it is recommended to carefully evaluate your options, as you don't want to risk losing your existing coverage.\n",
      "\n",
      "Always consider consulting with a healthcare navigator or a benefits counselor who can help you understand the implications and provide tailored advice based on your specific situation.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(trainer.train_dataset['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only Focus on the `Response Part` for the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=64): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21093/21093 [00:04<00:00, 4372.26 examples/s]\n",
      "Map (num_proc=64): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2344/2344 [00:02<00:00, 846.69 examples/s] \n"
     ]
    }
   ],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'gpt_response', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 21093\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 2746,\n",
       " 499,\n",
       " 527,\n",
       " 12588,\n",
       " 9983,\n",
       " 8398,\n",
       " 75368,\n",
       " 22413,\n",
       " 320,\n",
       " 1242,\n",
       " 18091,\n",
       " 8,\n",
       " 323,\n",
       " 527,\n",
       " 17446,\n",
       " 369,\n",
       " 31822,\n",
       " 11,\n",
       " 499,\n",
       " 656,\n",
       " 539,\n",
       " 1205,\n",
       " 311,\n",
       " 6994,\n",
       " 8821,\n",
       " 2890,\n",
       " 2512,\n",
       " 10401,\n",
       " 1234,\n",
       " 279,\n",
       " 43606,\n",
       " 10852,\n",
       " 3298,\n",
       " 320,\n",
       " 5581,\n",
       " 398,\n",
       " 14183,\n",
       " 311,\n",
       " 439,\n",
       " 330,\n",
       " 4213,\n",
       " 34924,\n",
       " 1865,\n",
       " 31822,\n",
       " 374,\n",
       " 6646,\n",
       " 8187,\n",
       " 7718,\n",
       " 10401,\n",
       " 11,\n",
       " 902,\n",
       " 3445,\n",
       " 430,\n",
       " 3515,\n",
       " 31822,\n",
       " 69001,\n",
       " 279,\n",
       " 2890,\n",
       " 8276,\n",
       " 16686,\n",
       " 1234,\n",
       " 279,\n",
       " 43606,\n",
       " 10852,\n",
       " 3298,\n",
       " 382,\n",
       " 11458,\n",
       " 11,\n",
       " 499,\n",
       " 1253,\n",
       " 5268,\n",
       " 311,\n",
       " 52880,\n",
       " 304,\n",
       " 264,\n",
       " 58358,\n",
       " 3197,\n",
       " 422,\n",
       " 499,\n",
       " 1390,\n",
       " 5217,\n",
       " 10401,\n",
       " 477,\n",
       " 422,\n",
       " 1070,\n",
       " 527,\n",
       " 3230,\n",
       " 3600,\n",
       " 430,\n",
       " 31822,\n",
       " 1587,\n",
       " 539,\n",
       " 3504,\n",
       " 13,\n",
       " 13538,\n",
       " 3339,\n",
       " 904,\n",
       " 4442,\n",
       " 11,\n",
       " 433,\n",
       " 374,\n",
       " 11349,\n",
       " 311,\n",
       " 15884,\n",
       " 15806,\n",
       " 701,\n",
       " 2671,\n",
       " 11,\n",
       " 439,\n",
       " 499,\n",
       " 1541,\n",
       " 956,\n",
       " 1390,\n",
       " 311,\n",
       " 5326,\n",
       " 13490,\n",
       " 701,\n",
       " 6484,\n",
       " 10401,\n",
       " 382,\n",
       " 38195,\n",
       " 2980,\n",
       " 31831,\n",
       " 449,\n",
       " 264,\n",
       " 18985,\n",
       " 36509,\n",
       " 477,\n",
       " 264,\n",
       " 7720,\n",
       " 62475,\n",
       " 889,\n",
       " 649,\n",
       " 1520,\n",
       " 499,\n",
       " 3619,\n",
       " 279,\n",
       " 25127,\n",
       " 323,\n",
       " 3493,\n",
       " 41891,\n",
       " 9650,\n",
       " 3196,\n",
       " 389,\n",
       " 701,\n",
       " 3230,\n",
       " 6671,\n",
       " 13,\n",
       " 128009]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The labels are created which only contain response. Left Padding is implemented and all the padding tokens are given a score of -100 to avoid loss calculation for pad_tokens\n",
    "trainer.train_dataset['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 21,093 | Num Epochs = 1 | Total steps = 2,637\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 97,255,424/3,310,005,248 (2.94% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='2637' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  84/2637 04:41 < 2:26:08, 0.29 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.472400</td>\n",
       "      <td>1.212031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer_stats = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:327\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/transformers/trainer.py:4588\u001b[39m, in \u001b[36mTrainer.floating_point_ops\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m   4575\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4576\u001b[39m \u001b[33;03mFor models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\u001b[39;00m\n\u001b[32m   4577\u001b[39m \u001b[33;03moperations for every backward + forward pass. If using another model, either implement such a method in the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4585\u001b[39m \u001b[33;03m    `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[32m   4586\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4587\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mfloating_point_ops\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloating_point_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4589\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4590\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/transformers/modeling_utils.py:1757\u001b[39m, in \u001b[36mModuleUtilsMixin.floating_point_ops\u001b[39m\u001b[34m(self, input_dict, exclude_embeddings)\u001b[39m\n\u001b[32m   1733\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfloating_point_ops\u001b[39m(\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28mself\u001b[39m, input_dict: Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch.Tensor, Any]], exclude_embeddings: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1735\u001b[39m ) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   1736\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1737\u001b[39m \u001b[33;03m    Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[33;03m    batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1754\u001b[39m \u001b[33;03m        `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[32m   1755\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1757\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m6\u001b[39m * \u001b[38;5;28mself\u001b[39m.estimate_tokens(input_dict) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/transformers/modeling_utils.py:1678\u001b[39m, in \u001b[36mModuleUtilsMixin.num_parameters\u001b[39m\u001b[34m(self, only_trainable, exclude_embeddings)\u001b[39m\n\u001b[32m   1673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exclude_embeddings:\n\u001b[32m   1674\u001b[39m     embedding_param_names = [\n\u001b[32m   1675\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.weight\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.named_modules() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module_type, nn.Embedding)\n\u001b[32m   1676\u001b[39m     ]\n\u001b[32m   1677\u001b[39m     total_parameters = [\n\u001b[32m-> \u001b[39m\u001b[32m1678\u001b[39m         parameter \u001b[38;5;28;01mfor\u001b[39;00m name, parameter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.named_parameters() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m embedding_param_names\n\u001b[32m   1679\u001b[39m     ]\n\u001b[32m   1680\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1681\u001b[39m     total_parameters = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.parameters())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/torch/nn/modules/module.py:2288\u001b[39m, in \u001b[36mModule.named_parameters\u001b[39m\u001b[34m(self, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2264\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[32m   2265\u001b[39m \n\u001b[32m   2266\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2283\u001b[39m \n\u001b[32m   2284\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2285\u001b[39m gen = \u001b[38;5;28mself\u001b[39m._named_members(\n\u001b[32m   2286\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m module: module._parameters.items(),\n\u001b[32m   2287\u001b[39m     prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)\n\u001b[32m-> \u001b[39m\u001b[32m2288\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/torch/nn/modules/module.py:2224\u001b[39m, in \u001b[36mModule._named_members\u001b[39m\u001b[34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2222\u001b[39m modules = \u001b[38;5;28mself\u001b[39m.named_modules(prefix=prefix, remove_duplicate=remove_duplicate) \u001b[38;5;28;01mif\u001b[39;00m recurse \u001b[38;5;28;01melse\u001b[39;00m [(prefix, \u001b[38;5;28mself\u001b[39m)]\n\u001b[32m   2223\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module_prefix, module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m-> \u001b[39m\u001b[32m2224\u001b[39m     members = \u001b[43mget_members_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2225\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m members:\n\u001b[32m   2226\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m memo:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/torch/nn/modules/module.py:2286\u001b[39m, in \u001b[36mModule.named_parameters.<locals>.<lambda>\u001b[39m\u001b[34m(module)\u001b[39m\n\u001b[32m   2258\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnamed_parameters\u001b[39m(\n\u001b[32m   2259\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2260\u001b[39m         prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   2261\u001b[39m         recurse: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2262\u001b[39m         remove_duplicate: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2263\u001b[39m ) -> Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, Parameter]]:\n\u001b[32m   2264\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[32m   2265\u001b[39m \n\u001b[32m   2266\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2283\u001b[39m \n\u001b[32m   2284\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2285\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m._named_members(\n\u001b[32m-> \u001b[39m\u001b[32m2286\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m module: module._parameters.items(),\n\u001b[32m   2287\u001b[39m         prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)\n\u001b[32m   2288\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just save the LoRA Adapters without merging with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_path = \"./llama32-sft-peft-kabatubare\" #use for LoRA based fine-tuning\n",
    "\n",
    "# Or run the two below statements\n",
    "model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_model_path = \"./llama32-sft-full-kabatubare\"\n",
    "peft_model_path = \"./llama32-sft-peft-kabatubare\" #use for LoRA based fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = peft_model_path,\n",
    "    max_seq_length = 4096,\n",
    "    load_in_4bit = False, # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    dtype=None, #None for auto-detection. Can be torch.bfloat16 or torch.float16 (will be automatically detected)\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "for idx in range(1,50):\n",
    "\n",
    "    print(dataset['test']['question'][idx])\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health\"},\n",
    "        {\"role\": \"user\", \"content\": dataset['test']['question'][idx]}]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(model.device)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=2048, num_return_sequences=1)\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(text.split(\"assistant\")[1])\n",
    "\n",
    "    print('---------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
